{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c825a9-d4cf-4634-8e69-f9df827208f8",
   "metadata": {},
   "source": [
    "# API demo for pax (lightweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786b4a10-e70f-42f2-b287-600118c883ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport pax\n",
    "\n",
    "from pax import Dense\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a302a1-8d5b-41b6-bc88-d8e679cc1460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bytes': b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   image  label\n",
       "0      {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      5\n",
       "1      {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      0\n",
       "2      {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      4\n",
       "3      {'bytes': b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      1\n",
       "4      {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      9\n",
       "...                                                  ...    ...\n",
       "59995  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      8\n",
       "59996  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      3\n",
       "59997  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      5\n",
       "59998  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      6\n",
       "59999  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      8\n",
       "\n",
       "[60000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/mnist/mnist_train.parquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5626870c-e184-4efe-9ee6-5a0404c35ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bytes_dict_to_jax_array(d):\n",
    "    img = Image.open(BytesIO(d['bytes']))\n",
    "    return jnp.array(img)\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df = df.copy()\n",
    "    df['image'] = df['image'].map(bytes_dict_to_jax_array)\n",
    "    X, y = jnp.stack(df['image'].tolist()), jax.nn.one_hot(df['label'], 10)\n",
    "    X = X.reshape(X.shape[0], -1) / 255\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = preprocess_df(df)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ff0c9e-eaa7-4ccc-8087-446bcc486e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer1 = Dense(in_nodes, out_nodes) # or whatever\n",
    "# layer2 = BatchNorm() # or whatever\n",
    "# ...\n",
    "# layerN = ...\n",
    "# layers = [layer1, ..., layerN]\n",
    "\n",
    "# params = [layer1.weights(), layer2.weights(), ...., layerN.weights()]\n",
    "# state = [layer1.state(), layer2.state(), ...., layerN.state()]\n",
    "# layer_funcs = [layer1.func(), layer2.func(), ..., layerN.func()]\n",
    "\n",
    "# @partial(jax.jit, static_argnames=\"is_training\")\n",
    "# def apply(params, state, X, *, is_training, key):\n",
    "#   for i in range(N):\n",
    "#     cur_key, key = jax.random.split(key)\n",
    "#     X, state[i] = layer_funcs[i](params[i], state[i], X, is_training=is_training, key=cur_key)\n",
    "#   return X, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9320ed-fa33-4f36-8788-9f3802d44cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dense(in_nodes=784, out_nodes=128, activation=<jax._src.custom_derivatives.custom_jvp object at 0x718553ceb050>),\n",
       " Dense(in_nodes=128, out_nodes=64, activation=<jax._src.custom_derivatives.custom_jvp object at 0x718553ceb050>),\n",
       " Dense(in_nodes=64, out_nodes=10, activation=<function Dense.__post_init__.<locals>.<lambda> at 0x718546c5af20>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes = [X_train.shape[1], 128, 64, 10]\n",
    "\n",
    "layers = []\n",
    "for i in range(len(shapes)-2):\n",
    "    layers.append(Dense(shapes[i], shapes[i+1], activation=jax.nn.relu))\n",
    "layers.append(Dense(shapes[-2], shapes[-1]))\n",
    "\n",
    "layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5643e9-caa5-49cd-88b1-586adf691d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef(({'b': *, 'w': *}, {'b': *, 'w': *}, {'b': *, 'w': *}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = tuple(l.weights(jax.random.key(0)) for l in layers)\n",
    "state = tuple(l.state() for l in layers)\n",
    "\n",
    "jax.tree.structure(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cc0005a-1c5c-46b4-b02e-60b8a9f999af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef({'b': *, 'w': *})\n",
      "PyTreeDef({'b': *, 'w': *})\n",
      "PyTreeDef({'b': *, 'w': *})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array([[-0.15497945, -0.4568455 , -0.39541277, ..., -0.13534677,\n",
       "          0.32761425, -0.19820786],\n",
       "        [-0.05561724, -0.49285766, -0.10636605, ..., -0.24057765,\n",
       "          0.07059948, -0.31488508],\n",
       "        [ 0.12788975, -0.14176412, -0.0644017 , ..., -0.2389179 ,\n",
       "         -0.296124  ,  0.12700762],\n",
       "        ...,\n",
       "        [ 0.13261902, -0.45547187, -0.2919717 , ..., -0.10995544,\n",
       "          0.2916181 , -0.04156661],\n",
       "        [ 0.17468776, -0.6099087 , -0.51634234, ...,  0.0476726 ,\n",
       "         -0.05541314, -0.24837567],\n",
       "        [ 0.103691  , -0.23499839, -0.41506302, ..., -0.2683155 ,\n",
       "         -0.00104973, -0.01717444]], dtype=float32),\n",
       " ({}, {}, {}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit\n",
    "def fwd(params, state, X, *, is_training, key):\n",
    "    for i, layer in enumerate(layers):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        print(jax.tree.structure(params[i]))\n",
    "        X, state_i = layer.func()(params[i], state[i], X, is_training=is_training, key=subkey)\n",
    "        state = state[:i] + (state_i,) + state[i+1:]  # update tuple immutably\n",
    "    return X, state\n",
    "\n",
    "fwd(params, state, X_train, is_training=False, key=jax.random.key(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "102f7f7e-5acd-4e21-9255-9641b61c9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, state, X, y, *, key):\n",
    "    logits, new_state = fwd(params, state, X, is_training=True, key=key)\n",
    "    loss = -jnp.mean(jnp.sum(y * jax.nn.log_softmax(logits), axis=1))\n",
    "    return loss, new_state\n",
    "\n",
    "(loss_val, new_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(params, state, X_train, y_train, key=jax.random.key(0))\n",
    "# loss_val, new_state, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "403cac3c-1402-4fc0-a0ec-38efb7491831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 2.317497968673706\n",
      "step 100, loss: 2.2239949703216553\n",
      "step 200, loss: 2.140049695968628\n",
      "step 300, loss: 2.0588788986206055\n",
      "step 400, loss: 1.9777990579605103\n",
      "step 500, loss: 1.8962042331695557\n",
      "step 600, loss: 1.814244031906128\n",
      "step 700, loss: 1.7326817512512207\n",
      "step 800, loss: 1.65241277217865\n",
      "step 900, loss: 1.574188470840454\n",
      "step 1000, loss: 1.4984387159347534\n",
      "step 1100, loss: 1.4256908893585205\n",
      "step 1200, loss: 1.3563148975372314\n",
      "step 1300, loss: 1.2907150983810425\n",
      "step 1400, loss: 1.2293421030044556\n",
      "step 1500, loss: 1.1722177267074585\n",
      "step 1600, loss: 1.119492769241333\n",
      "step 1700, loss: 1.0709500312805176\n",
      "step 1800, loss: 1.0264092683792114\n",
      "step 1900, loss: 0.9856464862823486\n",
      "step 2000, loss: 0.9482986927032471\n",
      "step 2100, loss: 0.9141151309013367\n",
      "step 2200, loss: 0.8827757835388184\n",
      "step 2300, loss: 0.8539860248565674\n",
      "step 2400, loss: 0.8275564312934875\n",
      "step 2500, loss: 0.8031908869743347\n",
      "step 2600, loss: 0.7807102799415588\n",
      "step 2700, loss: 0.7599031925201416\n",
      "step 2800, loss: 0.7406266927719116\n",
      "step 2900, loss: 0.7227067947387695\n",
      "step 3000, loss: 0.7060229778289795\n",
      "step 3100, loss: 0.6904810070991516\n",
      "step 3200, loss: 0.6758949160575867\n",
      "step 3300, loss: 0.6622583270072937\n",
      "step 3400, loss: 0.6494659185409546\n",
      "step 3500, loss: 0.6374235153198242\n",
      "step 3600, loss: 0.6260935068130493\n",
      "step 3700, loss: 0.6153959631919861\n",
      "step 3800, loss: 0.6053149700164795\n",
      "step 3900, loss: 0.5957494974136353\n",
      "step 4000, loss: 0.5866950750350952\n",
      "step 4100, loss: 0.5780938863754272\n",
      "step 4200, loss: 0.5699431300163269\n",
      "step 4300, loss: 0.5621895790100098\n",
      "step 4400, loss: 0.5548286437988281\n",
      "step 4500, loss: 0.5477701425552368\n",
      "step 4600, loss: 0.5410501956939697\n",
      "step 4700, loss: 0.5346195101737976\n",
      "step 4800, loss: 0.5284820199012756\n",
      "step 4900, loss: 0.522616982460022\n",
      "step 5000, loss: 0.5169727802276611\n",
      "step 5100, loss: 0.5115604996681213\n",
      "step 5200, loss: 0.5063757300376892\n",
      "step 5300, loss: 0.5013724565505981\n",
      "step 5400, loss: 0.49658873677253723\n",
      "step 5500, loss: 0.4919673204421997\n",
      "step 5600, loss: 0.4875030517578125\n",
      "step 5700, loss: 0.4832034707069397\n",
      "step 5800, loss: 0.47906145453453064\n",
      "step 5900, loss: 0.4750654697418213\n",
      "step 6000, loss: 0.471187949180603\n",
      "step 6100, loss: 0.4674644470214844\n",
      "step 6200, loss: 0.4638349115848541\n",
      "step 6300, loss: 0.46033602952957153\n",
      "step 6400, loss: 0.45694500207901\n",
      "step 6500, loss: 0.4536574184894562\n",
      "step 6600, loss: 0.45047083497047424\n",
      "step 6700, loss: 0.4473617374897003\n",
      "step 6800, loss: 0.44436103105545044\n",
      "step 6900, loss: 0.44144168496131897\n",
      "step 7000, loss: 0.4385949373245239\n",
      "step 7100, loss: 0.43582579493522644\n",
      "step 7200, loss: 0.43314647674560547\n",
      "step 7300, loss: 0.4305206835269928\n",
      "step 7400, loss: 0.4279857575893402\n",
      "step 7500, loss: 0.4254942834377289\n",
      "step 7600, loss: 0.42307811975479126\n",
      "step 7700, loss: 0.42071250081062317\n",
      "step 7800, loss: 0.41841667890548706\n",
      "step 7900, loss: 0.41617894172668457\n",
      "step 8000, loss: 0.41399022936820984\n",
      "step 8100, loss: 0.41185516119003296\n",
      "step 8200, loss: 0.4097649157047272\n",
      "step 8300, loss: 0.40771836042404175\n",
      "step 8400, loss: 0.4057193696498871\n",
      "step 8500, loss: 0.40376994013786316\n",
      "step 8600, loss: 0.40185651183128357\n",
      "step 8700, loss: 0.3999863862991333\n",
      "step 8800, loss: 0.3981570303440094\n",
      "step 8900, loss: 0.3963663578033447\n",
      "step 9000, loss: 0.39460960030555725\n",
      "step 9100, loss: 0.39287683367729187\n",
      "step 9200, loss: 0.3911908268928528\n",
      "step 9300, loss: 0.38953959941864014\n",
      "step 9400, loss: 0.3879103362560272\n",
      "step 9500, loss: 0.38632071018218994\n",
      "step 9600, loss: 0.38474738597869873\n",
      "step 9700, loss: 0.3832114636898041\n",
      "step 9800, loss: 0.3816972076892853\n",
      "step 9900, loss: 0.3802151083946228\n",
      "step 10000, loss: 0.37875300645828247\n",
      "step 10100, loss: 0.37731456756591797\n",
      "step 10200, loss: 0.3759019374847412\n",
      "step 10300, loss: 0.37451401352882385\n",
      "step 10400, loss: 0.3731422424316406\n",
      "step 10500, loss: 0.37180691957473755\n",
      "step 10600, loss: 0.3704850971698761\n",
      "step 10700, loss: 0.3691895306110382\n",
      "step 10800, loss: 0.3679051399230957\n",
      "step 10900, loss: 0.3666505217552185\n",
      "step 11000, loss: 0.3653954565525055\n",
      "step 11100, loss: 0.3641803562641144\n",
      "step 11200, loss: 0.362971693277359\n",
      "step 11300, loss: 0.36176836490631104\n",
      "step 11400, loss: 0.360602468252182\n",
      "step 11500, loss: 0.35944801568984985\n",
      "step 11600, loss: 0.358307808637619\n",
      "step 11700, loss: 0.35719218850135803\n",
      "step 11800, loss: 0.3560807406902313\n",
      "step 11900, loss: 0.35498690605163574\n",
      "step 12000, loss: 0.35390907526016235\n",
      "step 12100, loss: 0.3528542220592499\n",
      "step 12200, loss: 0.3518013656139374\n",
      "step 12300, loss: 0.35077372193336487\n",
      "step 12400, loss: 0.3497451841831207\n",
      "step 12500, loss: 0.34873855113983154\n",
      "step 12600, loss: 0.34774261713027954\n",
      "step 12700, loss: 0.34675976634025574\n",
      "step 12800, loss: 0.34578874707221985\n",
      "step 12900, loss: 0.34483101963996887\n",
      "step 13000, loss: 0.3438847064971924\n",
      "step 13100, loss: 0.3429458439350128\n",
      "step 13200, loss: 0.3420221507549286\n",
      "step 13300, loss: 0.34110018610954285\n",
      "step 13400, loss: 0.34020182490348816\n",
      "step 13500, loss: 0.3393094539642334\n",
      "step 13600, loss: 0.3384244740009308\n",
      "step 13700, loss: 0.3375537395477295\n",
      "step 13800, loss: 0.3366903066635132\n",
      "step 13900, loss: 0.335833340883255\n",
      "step 14000, loss: 0.3349918723106384\n",
      "step 14100, loss: 0.3341561257839203\n",
      "step 14200, loss: 0.33333373069763184\n",
      "step 14300, loss: 0.33250871300697327\n",
      "step 14400, loss: 0.33169928193092346\n",
      "step 14500, loss: 0.3309060335159302\n",
      "step 14600, loss: 0.330109179019928\n",
      "step 14700, loss: 0.32932722568511963\n",
      "step 14800, loss: 0.3285471498966217\n",
      "step 14900, loss: 0.32777565717697144\n",
      "step 15000, loss: 0.3270128667354584\n",
      "step 15100, loss: 0.32625654339790344\n",
      "step 15200, loss: 0.325509637594223\n",
      "step 15300, loss: 0.32476556301116943\n",
      "step 15400, loss: 0.3240330219268799\n",
      "step 15500, loss: 0.3233036398887634\n",
      "step 15600, loss: 0.3225756883621216\n",
      "step 15700, loss: 0.3218687176704407\n",
      "step 15800, loss: 0.32115885615348816\n",
      "step 15900, loss: 0.32045918703079224\n",
      "step 16000, loss: 0.3197665512561798\n",
      "step 16100, loss: 0.3190694749355316\n",
      "step 16200, loss: 0.3183901309967041\n",
      "step 16300, loss: 0.31771132349967957\n",
      "step 16400, loss: 0.31704309582710266\n",
      "step 16500, loss: 0.31638216972351074\n",
      "step 16600, loss: 0.31572431325912476\n",
      "step 16700, loss: 0.31507131457328796\n",
      "step 16800, loss: 0.3144189119338989\n",
      "step 16900, loss: 0.31377971172332764\n",
      "step 17000, loss: 0.31313931941986084\n",
      "step 17100, loss: 0.3125021457672119\n",
      "step 17200, loss: 0.31187349557876587\n",
      "step 17300, loss: 0.3112483024597168\n",
      "step 17400, loss: 0.3106357455253601\n",
      "step 17500, loss: 0.31002286076545715\n",
      "step 17600, loss: 0.30941295623779297\n",
      "step 17700, loss: 0.3088121712207794\n",
      "step 17800, loss: 0.308210164308548\n",
      "step 17900, loss: 0.30761468410491943\n",
      "step 18000, loss: 0.3070240616798401\n",
      "step 18100, loss: 0.3064376711845398\n",
      "step 18200, loss: 0.3058593273162842\n",
      "step 18300, loss: 0.30527937412261963\n",
      "step 18400, loss: 0.30470579862594604\n",
      "step 18500, loss: 0.30413535237312317\n",
      "step 18600, loss: 0.3035714328289032\n",
      "step 18700, loss: 0.3030083477497101\n",
      "step 18800, loss: 0.30244988203048706\n",
      "step 18900, loss: 0.30190014839172363\n",
      "step 19000, loss: 0.30135005712509155\n",
      "step 19100, loss: 0.30080196261405945\n",
      "step 19200, loss: 0.3002593219280243\n",
      "step 19300, loss: 0.29972246289253235\n",
      "step 19400, loss: 0.29918795824050903\n",
      "step 19500, loss: 0.29865437746047974\n",
      "step 19600, loss: 0.29812753200531006\n",
      "step 19700, loss: 0.29760032892227173\n",
      "step 19800, loss: 0.297084778547287\n",
      "step 19900, loss: 0.2965652346611023\n",
      "step 20000, loss: 0.2960475981235504\n",
      "step 20100, loss: 0.29554101824760437\n",
      "step 20200, loss: 0.29503047466278076\n",
      "step 20300, loss: 0.29453158378601074\n",
      "step 20400, loss: 0.294026643037796\n",
      "step 20500, loss: 0.29353365302085876\n",
      "step 20600, loss: 0.29303178191185\n",
      "step 20700, loss: 0.29253730177879333\n",
      "step 20800, loss: 0.29205939173698425\n",
      "step 20900, loss: 0.2915720045566559\n",
      "step 21000, loss: 0.2910884916782379\n",
      "step 21100, loss: 0.2906087338924408\n",
      "step 21200, loss: 0.2901361584663391\n",
      "step 21300, loss: 0.2896602153778076\n",
      "step 21400, loss: 0.2891918122768402\n",
      "step 21500, loss: 0.28871986269950867\n",
      "step 21600, loss: 0.2882520258426666\n",
      "step 21700, loss: 0.28779786825180054\n",
      "step 21800, loss: 0.28733110427856445\n",
      "step 21900, loss: 0.28687286376953125\n",
      "step 22000, loss: 0.2864213287830353\n",
      "step 22100, loss: 0.2859598994255066\n",
      "step 22200, loss: 0.2855130136013031\n",
      "step 22300, loss: 0.2850688099861145\n",
      "step 22400, loss: 0.28461790084838867\n",
      "step 22500, loss: 0.2841759920120239\n",
      "step 22600, loss: 0.28373977541923523\n",
      "step 22700, loss: 0.28329771757125854\n",
      "step 22800, loss: 0.2828602194786072\n",
      "step 22900, loss: 0.2824293076992035\n",
      "step 23000, loss: 0.2820003628730774\n",
      "step 23100, loss: 0.28156569600105286\n",
      "step 23200, loss: 0.28114044666290283\n",
      "step 23300, loss: 0.28071290254592896\n",
      "step 23400, loss: 0.28028976917266846\n",
      "step 23500, loss: 0.2798719108104706\n",
      "step 23600, loss: 0.2794475257396698\n",
      "step 23700, loss: 0.27903133630752563\n",
      "step 23800, loss: 0.2786159813404083\n",
      "step 23900, loss: 0.27820658683776855\n",
      "step 24000, loss: 0.27779102325439453\n",
      "step 24100, loss: 0.27738380432128906\n",
      "step 24200, loss: 0.27697741985321045\n",
      "step 24300, loss: 0.27657410502433777\n",
      "step 24400, loss: 0.2761654853820801\n",
      "step 24500, loss: 0.27576470375061035\n",
      "step 24600, loss: 0.2753666639328003\n",
      "step 24700, loss: 0.2749699354171753\n",
      "step 24800, loss: 0.2745768129825592\n",
      "step 24900, loss: 0.2741853594779968\n",
      "step 25000, loss: 0.27379199862480164\n",
      "step 25100, loss: 0.2734021842479706\n",
      "step 25200, loss: 0.27301025390625\n",
      "step 25300, loss: 0.2726246416568756\n",
      "step 25400, loss: 0.272245317697525\n",
      "step 25500, loss: 0.27186286449432373\n",
      "step 25600, loss: 0.2714810371398926\n",
      "step 25700, loss: 0.27110326290130615\n",
      "step 25800, loss: 0.2707284986972809\n",
      "step 25900, loss: 0.27034664154052734\n",
      "step 26000, loss: 0.26997387409210205\n",
      "step 26100, loss: 0.2696044147014618\n",
      "step 26200, loss: 0.2692314386367798\n",
      "step 26300, loss: 0.26886147260665894\n",
      "step 26400, loss: 0.2684928774833679\n",
      "step 26500, loss: 0.2681308090686798\n",
      "step 26600, loss: 0.2677633464336395\n",
      "step 26700, loss: 0.26739969849586487\n",
      "step 26800, loss: 0.26703667640686035\n",
      "step 26900, loss: 0.2666813135147095\n",
      "step 27000, loss: 0.2663230299949646\n",
      "step 27100, loss: 0.2659672498703003\n",
      "step 27200, loss: 0.2656092047691345\n",
      "step 27300, loss: 0.26524943113327026\n",
      "step 27400, loss: 0.2648961544036865\n",
      "step 27500, loss: 0.26454901695251465\n",
      "step 27600, loss: 0.26419341564178467\n",
      "step 27700, loss: 0.26384642720222473\n",
      "step 27800, loss: 0.26350274682044983\n",
      "step 27900, loss: 0.26314860582351685\n",
      "step 28000, loss: 0.2628057599067688\n",
      "step 28100, loss: 0.2624630928039551\n",
      "step 28200, loss: 0.26212140917778015\n",
      "step 28300, loss: 0.2617839276790619\n",
      "step 28400, loss: 0.2614395320415497\n",
      "step 28500, loss: 0.2610999345779419\n",
      "step 28600, loss: 0.2607601284980774\n",
      "step 28700, loss: 0.2604215741157532\n",
      "step 28800, loss: 0.2600870132446289\n",
      "step 28900, loss: 0.25976109504699707\n",
      "step 29000, loss: 0.25942787528038025\n",
      "step 29100, loss: 0.25909411907196045\n",
      "step 29200, loss: 0.2587622404098511\n",
      "step 29300, loss: 0.25843557715415955\n",
      "step 29400, loss: 0.2581036686897278\n",
      "step 29500, loss: 0.25778430700302124\n",
      "step 29600, loss: 0.25745677947998047\n",
      "step 29700, loss: 0.2571362257003784\n",
      "step 29800, loss: 0.2568094730377197\n",
      "step 29900, loss: 0.25648584961891174\n"
     ]
    }
   ],
   "source": [
    "# grad_loss = jax.jit(jax.grad(loss))\n",
    "\n",
    "@jax.jit\n",
    "def train(params, state, X_train, y_train, *, key, num_iters=30000):\n",
    "    lr = 0.001\n",
    "    keys = jax.random.split(key, num_iters)\n",
    "\n",
    "    def body(i, carry):\n",
    "        (params, state) = carry\n",
    "        (loss_val, new_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(params, state, X_train, y_train, key=keys[i])\n",
    "        params = jax.tree.map(lambda param, grad: param - lr * grad, params, grads)\n",
    "        \n",
    "        def do_print(_):\n",
    "            jax.debug.print(\"step {i}, loss: {l}\", i=i, l=loss_val)\n",
    "            return None\n",
    "\n",
    "        _ = jax.lax.cond(i % 100 == 0, do_print, lambda _: None, operand=None)\n",
    "        return params, new_state\n",
    "\n",
    "    params, final_state = jax.lax.fori_loop(0, num_iters, body, (params, state))\n",
    "    return params\n",
    "\n",
    "params = train(params, state, X_train, y_train, key=jax.random.key(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87d0df-1319-4a7d-a247-9e634771af9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
